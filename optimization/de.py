# coding: utf-8

__author__ = 'MÃ¡rio Antunes'
__version__ = '0.1'
__email__ = 'mariolpantunes@gmail.com'
__status__ = 'Development'


import enum
import math
import random
import typing
import joblib
import logging
import tempfile
import statistics
import numpy as np

from tqdm import tqdm


import optimization.utils as utils


logger = logging.getLogger(__name__)

@enum.unique
class TargetVector(enum.Enum):
    best = 'best'
    rand = 'rand'

    def __str__(self):
        return self.value


@enum.unique
class CrossoverMethod(enum.Enum):
    bin = 'bin'
    exp = 'exp'

    def __str__(self):
        return self.value


# define mutation operation
def mutation(x, F):
    diff = np.empty(x[0].shape)
    for i in range(1, len(x), 2):
        diff += x[i] - x[i+1]
    return x[0] + F * diff


def idx_bin(dims, cr):
    j = random.randrange(dims)
    idx = [True if random.random() < cr or i == j else False for i in range(dims)]
    return idx


def idx_exp(dims, cr):
    idx = []
    j = random.randrange(dims)
    idx.append(j)
    j = (j + 1) % dims
    while random.random() < cr and len(idx) < dims:
        idx.append(j)
        j = (j + 1)
    rv = [True if i in idx else False for i in range(dims)]
    return rv


def crossover(mutated, target, dims, cr, cross_method):
    idx = cross_method(dims, cr)
    trial = [mutated[i] if idx[i] else target[i] for i in range(dims)]
    return trial


def differential_evolution(objective:typing.Callable, bounds:np.ndarray, population:np.ndarray=None, 
callback:typing.Callable=None, variant='best/1/bin', n_iter:int=200, n_pop:int=20,
F=0.5, cr=0.7, rt=10, n_jobs=-1, cached=False, debug=False, verbose=False):
    try:
        v = variant.split('/')
        tv = TargetVector[v[0]]
        dv = int(v[1])
        cm = CrossoverMethod[v[2]]

        nc = 2*dv if tv is TargetVector.best else 2*dv+1
    except:
        raise ValueError('variant must be = [rand|best]/n/[bin|exp]')

    cross_method = {CrossoverMethod.bin: idx_bin, CrossoverMethod.exp: idx_exp}

    # cache the initial objective function
    if cached:
        # Cache from joblib
        location = tempfile.gettempdir()
        memory = joblib.Memory(location, verbose=0)
        objective_cache = memory.cache(objective)
    else:
        objective_cache = objective
    
    # check if the initial population is given
    if population is None:
        # initialise population of candidate solutions randomly within the specified bounds
        pop = bounds[:, 0] + (np.random.rand(n_pop, len(bounds)) * (bounds[:, 1] - bounds[:, 0]))
        pop = [utils.check_bounds(p, bounds) for p in pop]
        # evaluate initial population of candidate solutions
        obj_all = joblib.Parallel(n_jobs=n_jobs, backend='loky')(joblib.delayed(objective_cache)(c) for c in pop)
        # improve the quality of the initial solutions (avoid initial solutions with inf cost)
        r = 0
        while any(math.isinf(i) for i in obj_all) and r < rt:
            for i in range(n_pop):
                if math.isinf(obj_all[i]):
                    pop = bounds[:, 0] + (np.random.rand(n_pop, len(bounds)) * (bounds[:, 1] - bounds[:, 0]))
                    pop = [utils.check_bounds(p, bounds) for p in pop]
            obj_all = joblib.Parallel(n_jobs=n_jobs, backend='loky')(joblib.delayed(objective_cache)(c) for c in pop)
            r += 1
        # if after R repetitions it still has inf. cost
        if any(math.isinf(i) for i in obj_all):
            valid_idx = [i for i in range(n_pop) if not math.isinf(obj_all[i])]
            pop = pop[valid_idx]
            obj_all = [obj_all[i] for i in valid_idx]
            n_pop = len(valid_idx)
    else:
        # initialise population of candidate and validate the bounds
        pop = [utils.check_bounds(p, bounds) for p in population]
        # evaluate initial population of candidate solutions
        obj_all = joblib.Parallel(n_jobs=n_jobs, backend='loky')(joblib.delayed(objective_cache)(c) for c in pop)

    # find the best performing vector of initial population
    best_vector = pop[np.argmin(obj_all)]
    best_obj = min(obj_all)
    prev_obj = best_obj
    
    # arrays to store the debug information
    if debug:
        obj_avg_iter = []
        obj_best_iter = []
        obj_worst_iter = []
    
    # run iterations of the algorithm
    for epoch in tqdm(range(n_iter), disable=not verbose):
        # generate offspring
        offspring = []
        for j in tqdm(range(n_pop), leave=False, disable=not verbose):
            # choose three candidates, a, b and c, that are not the current one
            candidates_idx = random.choices([candidate for candidate in range(n_pop) if candidate != j], k = nc)
            diff_candidates = [pop[i] for i in candidates_idx]
            
            if tv is TargetVector.best:
                candidates = [best_vector]
                candidates.extend(diff_candidates)
            else:
                candidates = diff_candidates

            # perform mutation
            mutated = mutation(candidates, F)
            # check that lower and upper bounds are retained after mutation
            #mutated = utils.check_bounds(mutated, bounds)
            # perform crossover
            trial = crossover(mutated, pop[j], len(bounds), cr, cross_method[cm])
            offspring.append(trial)
        offspring = [utils.check_bounds(trial, bounds) for trial in offspring]
        obj_trial = joblib.Parallel(n_jobs=n_jobs, backend='loky')(joblib.delayed(objective_cache)(c) for c in offspring)

        # iterate over all candidate solutions
        for j in range(n_pop):
            # perform selection
            if obj_trial[j] < obj_all[j]:
                # replace the target vector with the trial vector
                pop[j] = offspring[j]
                # store the new objective function value
                obj_all[j] = obj_trial[j]
        # find the best performing vector at each iteration
        best_obj = min(obj_all)
        # store the lowest objective function value
        if best_obj < prev_obj:
            best_vector = pop[np.argmin(obj_all)]
            prev_obj = best_obj
        
        ## Optional execute the callback code
        if callback is not None:
            callback(epoch, obj_all)

        ## Optional store the debug information
        if debug:
            # store best, wort and average cost for all candidates
            obj_avg_iter.append(statistics.mean(obj_all))
            obj_best_iter.append(best_obj)
            obj_worst_iter.append(max(obj_all))
    # clean the cache
    if cached:
        memory.clear(warn=False)
    
    if debug:
        return (best_vector, best_obj, (obj_best_iter, obj_avg_iter, obj_worst_iter))
    else:
        return (best_vector, best_obj)